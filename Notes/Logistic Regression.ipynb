{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main difference between regression and classification is the response variable which is qualitative in classfication case.\n",
    "- Just as in regression, we have (x,y) training observations that we can use to build a classifier.\n",
    "\n",
    "- Widely used classifiers : Logistic regression, Linear Discriminant Analysis and K- Nearest Neighbours.\n",
    "\n",
    "- Logistic regression models the probability that Y belongs to a particular category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model a relationship between $p(X) = Pr(Y = 1 | X )$ and $X$\n",
    "\n",
    "- Logistic function (or Sigmoid function) : \n",
    "\n",
    "$p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{{1 + e^{\\beta_0 + \\beta_1 X} }}$\n",
    "\n",
    "or\n",
    "\n",
    "$p(X) = \\frac{1}{{1 + e^{-(\\beta_0 + \\beta_1 X)} }}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic function / Sigmoid Function\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit of manipulation we get:\n",
    "\n",
    "$\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}$\n",
    "\n",
    "\n",
    "- The quantity $\\frac{p(X)}{1 - p(X)}$ is called odds and can take any value between 0 and inf.\n",
    "- Example, if p = 0.9, implies an odd of 9 or '9 out of 10 people will default'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By taking log on both side of the equation:\n",
    "\n",
    "$log(\\frac{p(X)}{1-p(X)}) = \\beta_0 + \\beta_1 X$ \n",
    "\n",
    "- This is also called log-odds or logit.\n",
    "\n",
    "- In contrast to regression, here 1 unit increase in X changes the log odds by $\\beta_1$. However, $\\beta_1$ does not correspond to change in $p(X)$ associated with a one-unit increase in X since the relationship between X and $p(X)$ is not a straight line.\n",
    "\n",
    "- The amount of $p(X)$ change due to a unit increase in X will depend on current value of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Coefficients :\n",
    "\n",
    "- For determining the coefficients $\\beta_0$ and $\\beta_1$ we will use maximum likelihood method.\n",
    "\n",
    "- In this method we try to find the values of $\\beta_0$ and $\\beta_1$ such that plugging these values into the model gives a number close to 1 if defaulted and close to 0 if not defaulted.\n",
    "\n",
    "- Estimates $\\beta_0$ and $\\beta_1$ are chosen to maximize the likelihood function. If f is logistic function,  $f(x, \\beta)$ is the probability when y is 1 and $1-f(x, \\beta)$ when y is 0.\n",
    "\n",
    "- Hence, maximum likelihood function is given as:\n",
    "\n",
    "$L(\\beta_0, \\beta_1) = f(x, \\beta)^y * (1-f(x, \\beta))^{1-y} $\n",
    "\n",
    "- It's simpler to calculate the log of this function\n",
    "$log L(\\beta_0, \\beta_1) = ylogf(x, \\beta) + (1-y)log(1-f(x, \\beta)) $\n",
    "\n",
    "- If we assume different data points are independent from one another, the overall likelihood is just the product of the individual likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's derivative is given by: (Prob density function/ Likelihood function)\n",
    "def logistic_prime(x):\n",
    "    return logistic(x) * (1 - logistic(x))\n",
    "\n",
    "# Maximize likelihood function\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1- logistic(dot(x_i, beta)))\n",
    "    \n",
    "\n",
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "              for x_i, y_i in zip(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict a binary response using multiple predictors.\n",
    "\n",
    "- By analogy we can generalie this formula:\n",
    "\n",
    "$log(\\frac{p(X)}{1-p(X)}) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p$\n",
    "\n",
    "- and $p(X)$ becomes :\n",
    "\n",
    "$p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 +.. + \\beta_p X_p}}{{1 + e^{\\beta_0 + \\beta_1 X_1 +...+ \\beta_p X_p} }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Gradient\n",
    "\n",
    "- Now, we need to calculate the gradient of the log of likelihood function\n",
    "\n",
    "- $\\beta x$ is the dot product of the vectors $\\beta$ and x, or simply a weighted sum of the components of x (with $\\beta$ containing the weights).\n",
    "\n",
    "- To maximize the likelihood we need to simply choose the value of $\\beta$ which maximize it. This can be done using an optimization algorithm. \n",
    "\n",
    "- First we need partial derivative of likelihood with respect to each parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"here is the index of the data point,\n",
    "    j is the index of the derivative\"\"\"\n",
    "    \n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "# Calculating the gradient\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"gradient of log of likelihood function\n",
    "    corresponding to the ith data point\"\"\"\n",
    "    \n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j)\n",
    "           for j,_ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add, \n",
    "                 [logistic_log_gradient_i(x_i, y_i, beta)\n",
    "                 for x_i, y_i in zip(x,y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "- We have to determine if a user has a premium account based on work exprience and salary.\n",
    "- 200 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix of data in which each row is a list[experience, salary, paid_account]\n",
    "\n",
    "x = [[1] + row[:2] for row in data] #each element is [1, experience, salary]\n",
    "y = [row[2] for row in data] # each element is paid_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PaidAccount = \\beta_0 + \\beta_1 experience + \\beta_2 salary +\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_x = rescale(x)\n",
    "beta = estimate_beta(rescale_x, y) #[0.26, 0.43, -0.43]\n",
    "predictions = [predict(x_i, beta) for x_i in rescaled_x]\n",
    "\n",
    "plt.scatter(predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "X_train, y_train, x_test, y_test = train_test_split(rescaled_x, y, 0.33)\n",
    "\n",
    "# Want to maximize the log likelihood on the training data\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "gradient_fn = partial(logistic_log_gradient, x_train, y_train)\n",
    "\n",
    "# Pick a random starting point\n",
    "beta_0 = [random.random() for _ in range(3)]\n",
    "\n",
    "# Maximize using gradient descent\n",
    "beta_hat = maximize_batch (fn, gradient_fn, beta_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
