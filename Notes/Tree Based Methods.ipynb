{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For both regression and classification.\n",
    "- These involve segmenting the predictor space into a number of simple regions.\n",
    "- In order to make a prediction we typically use mean or mode of the training observation in the region to which it belongs.\n",
    "- These set of splitting rules, used to segment the predictor space, can be summarized in a tree.\n",
    "- Tree based methods are useful for interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The regions are known as terminal nodes or leaves.\n",
    "- The points along the tree where the predictor space splits are referred to as internal nodes.\n",
    "- The segment of trees which connect the nodes are branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction via stratification of the Feature/Predictor space:\n",
    "\n",
    "1. We divide the predictor space - i.e, the set of possible values of $X_1, X_2, ..$ into j distinct and non-overlapping regions $R_1, R_2, ...$.\n",
    "2. For every observation that falls into region $R_j$ we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to construct the regions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal is to find regions which minimize the RSS (Residual Sum of Squares).\n",
    "\n",
    "    $\\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y_{R_j}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unfortunately it is computationally infeasible to consider every possible partition of the feature space into  boxes.\n",
    "- For this reason, we use the top-down greedy approach, known as recursive binary splitting. \n",
    "- Best split is made at that particular step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Recursive splitting\n",
    "\n",
    "- We first select the predictor $X_j$ and the cutpoint s such that splitting the predictor space into the regions ${X|X_j < s}$ and ${X|X_j >= s}$ leads to greatest possible reduction in RSS. \n",
    "- The notation ${X|X_j < s}$ means the region of predictor space in which $X_j$ takes on a value less than s.\n",
    "\n",
    "- For any j and s, we define pair of half-planes as\n",
    "\n",
    "    $R_1 (j,s) = {X|X_j < s}$ and $R_2 (j,s) = {X|X_j >= s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And we seek the value of j and s to minimize the equation,\n",
    "\n",
    "    $\\sum_{i:x_i \\in R_1(j,s)} (y_i - \\hat{y_{R_1}})^2 + \\sum_{i:x_i \\in R_2(j,s)} (y_i - \\hat{y_{R_2}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we repeat the process, looking for best predictor and best cutpoint in order to split the data further to minimize the RSS within each of the resulting regions.\n",
    "- However, this time, instead of splitting the whole predictor space, we split one of any two previously identified regions.\n",
    "- Once the regions are created, we predict the response for a given test observation using the mean of the training observations in the region to which that observation belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above process may produce good predictions but is likely to overfit the data.\n",
    "- One possible way is to build the tree so long as the decrease in the RSS due to each split exceeds certain threshold.\n",
    "- This strategy may result in smaller trees but is too short-sighted since a seemingly worthless split early on tree might be followed by a good split(i.e it leads to large reduction in RSS). \n",
    "- A better strategy is to grow a very large tree and then 'prune' it back in order to obtain a subtree. Our goal is to select a subtree that leads to lower error rate(using cross-validation).\n",
    "- However, estimating the cross-validation would be too cumbersome since there will be large number of possible subtrees.\n",
    "- Rather than considering every subtree, we consider a sequence of trees indexed by a non-negative number $\\alpha$.\n",
    "- For each value of $\\alpha$ there corresponds a subtree such that \n",
    "    \n",
    "    $\\sum_{m =1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat{y_{R_m}})^2 + \\alpha |T| $ \n",
    "    \n",
    "   \n",
    "- is as small as possible. $|T|$ indicates number of terminal nodes, $R_m$ is the rectangle (subset of predictor space) coresponding to the mth terminal node and $\\hat{y_{R_m}}$ is the predicted response associated with $R_m$ - That is mean of the training observations $R_m$.\n",
    "- When $\\alpha = 0$, then subtree T simply equals $T_0$ \n",
    "- As we increase $\\alpha$ from zero, branches get pruned from the tree in a nested and predictable fashion.\n",
    "\n",
    "Algorithm: \n",
    "\n",
    "1. Use binary recursive splitting to grow a large tree.\n",
    "\n",
    "2. Apply cost complexity pruning to the large tree to obtain a sequence of best subtrees as a function of $\\alpha$.\n",
    "\n",
    "3. Use k-fold cross-validation to choose $\\alpha$. For each k = 1, 2...K:\n",
    "    - Repeat steps 1 and 2 on all but kth fold.\n",
    "    - Evaluate the mean squared prediction error \n",
    "    - Average the results for each value of $\\alpha$ and pick one to minimize average error.\n",
    "    \n",
    "4. Return the subtree from step 2 that corresponds to choosen value of $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Very similar to regression trees, except that it is used to predict a qualitative response.\n",
    "- Here, we predict that each observation belongs to most commonly occurring class of training observations in the region to which it belongs.\n",
    "- We are also interested in class proportions.\n",
    "- We use recursive binary splitting to grow a classification tree.\n",
    "- We use classification error rate instead of RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification error rate is simply the fraction of training observations in that region that do not belong ot most common class:\n",
    "\n",
    "    $E = 1 - max_{k} (\\hat{p_{mk}})$\n",
    "    \n",
    "    \n",
    "- Here $\\hat{p_{mk}}$ represents the proportion of training observations in the mth region that are from kth class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "- However it is not sufficiently sensitive for tree-growing.\n",
    "- Another measure is 'Gini Index'\n",
    "\n",
    "    $G = \\sum_{k=1}^{K} \\hat{p_{mk}}(1-\\hat{p_{mk}})$\n",
    "    \n",
    "    \n",
    "- It is a measure of total variance across k classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "- An alternative to Gini index is Entropy. \n",
    "\n",
    "    $ D = -\\sum_{k=1}^{K} \\hat{p_{mk}} \\log \\hat{p_{mk}} $\n",
    "    \n",
    "- One can show that entropy will take on a value near zero if $\\hat{p_{mk}}$'s are all near zero or near one.\n",
    "- Therefore, entropy (like Gini index) will take on a small value if the mth node is pure.\n",
    "- They are used to evaluate the quality of a particular split.When a data point is close to one class the entropy is low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If all the data points belong to a single class, then there is no real uncertainity, there will be low entropy i/e $p_i$ is close to 0 or 1\n",
    "- If the data points are evenly spread across the classes, there is a lot of uncertainity and hence high entropy i.e $p_i$'s will be far from 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy \n",
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p,2)\n",
    "                for p in class_probabilities\n",
    "                if p)          # ignore zero probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "           for count in Counter(labels).values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we partition our data S into subsets $S_1$, $S_2$ .. containing proportions $q_1...q_m$, then we compute the entropy of partition as a weighted sum: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      $H = q_1 H(S_1) + ..... + q_m H(S_m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\n",
    "    subsets is a list of lists of labeled data(eg. true/false labels)\"\"\"\n",
    "    \n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    \n",
    "    return sum(data_entropy(subset) * len(subset) / total_count\n",
    "              for subset in subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data by each of attributes\n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"each input is a pair (attribute_dict, label).\n",
    "    returns a dict : attribute_value -> inputs\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = input[0][attribute] # get the value of the specified attribute\n",
    "        groups[key].append(input) # then add this input to the correct list\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which computes entropy for each partition\n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-82e87c26f4c5>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-82e87c26f4c5>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print key, partition_entropy_by(inputs, key)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Choose partition with minimum entropy\n",
    "for key in ['level', 'lang', 'tweets', 'phd']:\n",
    "    print key, partition_entropy_by(inputs, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest entropy from 'level' so we make a subtree for each possible 'level' value\n",
    "senior_inputs = [(input, label)\n",
    "                for input, label in inputs if input['level'] == \"Senior\"]\n",
    "\n",
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print key, partition_entropy_by(senior_inputs. key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decision trees suffer from high variance.\n",
    "- This means that if we split training data into 2 parts at random and fit a decion tree to both halves, the results that we get could be quite different.\n",
    "- Boostrap aggregation or 'bagging' is a general purpose procedure for reducing the variance of a statistical learning method.\n",
    "- Averaging a set of observations reduces variance.\n",
    "- A natural way to reduce variance is to take many training sets from population.\n",
    "- We could calculate $\\hat{f^1}(x), \\hat{f^2}(x)...$ using B training sets and average them in order to obtain a single low-variance statistical learning model.\n",
    "\n",
    "    $\\hat{f}_{avg}(x) = \\frac{1}{B} \\sum_{b=1}^{B}\\hat{f}^b(x)$\n",
    "    \n",
    "\n",
    "- This is not practical because we generally do not have access to multiple training sets. Instead we can bootstrap by taking repeated samples from a single training set. \n",
    "- In this approach we generate B different bootstrapped training sets. We then train our model on bth bootstrapped training set to get $\\hat{f^{*b}}(x)$ and finally average predictions to obtain:\n",
    "    $\\hat{f_{bag}(x)}= \\frac{1}{B} \\sum_{b=1}^{B}\\hat{f^{b}}(x)$\n",
    "    \n",
    "   \n",
    "- This is called bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for regression\n",
    "\n",
    "- To apply bagging to regression trees we construct B regression trees using B bootstrapped training sets and average the resulting predictions.\n",
    "- These trees are grown deep and not pruned.\n",
    "- Averaging the B trees reduces variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for classification\n",
    "\n",
    "- Simplest way is to record the class of test observation predicted by bunch of B trees and take majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- They provide an improvement over bagged tree by way of a small tweak that decorrelates the trees.\n",
    "- As in bagging, we build a number of decision trees based on bootstrapping training samples.\n",
    "- But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from full set of p predictors.\n",
    "- The split is allowed to use only one of those m predictors.\n",
    "- A fresh sample of m predictors is taken at each split and typically we choose $ m = \\sqrt{p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main difference between Random forest and bagging is the choice of predictor size m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another approach for improving the predictions resulting from a decision tree.\n",
    "- Trees are grown sequentially. Each tree is grown using information from previously grown trees.\n",
    "- Boosting does not involve bootstrap sampling, instead each tree is fit on a modified version of the orginal data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm for boosting:\n",
    "\n",
    "    1. Set the $\\hat{f}(x) = 0$ and $r_i = y_i$ for all i in the training set.\n",
    "\n",
    "    2. For b=1,2,...B repeat:\n",
    "        1. Fit a tree $\\hat{f^b}$ with d splits (d+1 terminal nodes) to the training data (X,r).\n",
    "        2. Update $\\hat{f}$ by adding in a shrunken version of the new tree:\n",
    "    \n",
    "            $\\hat{f}(x) = \\hat{f}(x) + \\lambda \\hat{f^b}(x)$\n",
    "        \n",
    "        3. Update the residuals\n",
    "        \n",
    "            $r_i = r_i - \\lambda \\hat{f^b}(x)$\n",
    "        \n",
    "    3. Output the boosted model\n",
    "\n",
    "         $\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f^b}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The boosting approach learns slowly.\n",
    "- We fit a tree to the residuals from the model rather than the outcome Y as response. We then add this new decision tree into the fitted function in order to update the residuals. \n",
    "- Each of these trees can be small with ust few terminal nodes determined by parameter 'd' in the algorithm.\n",
    "- By fitting small trees to the residuals, we slowly improve $\\hat{f}$ in areas where it does not perform well.\n",
    "- The shrinkage parameter $\\lambda$ slows the process down even further allowing more and different shaped trees to attack residuals. \n",
    "- The cosntruction of each tree depends strongly on trees that have already been grown (unlike in bagging). \n",
    "\n",
    "- Tuning parameters:\n",
    "    1. Number of trees B: Boosting can overfit if B is too large.\n",
    "    2. Shrinkage parameter $\\lambda$ a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001.\n",
    "    3. Number d of splits in each tree which controls the complexity of the boosted ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
