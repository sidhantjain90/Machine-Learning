{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When faced with a large number of correlated variables, principal components allow us to summarize the data set with smaller number of representative variables that collectively explain most of the variablility in the original set.\n",
    "- PCA refers to the process by which the principal components are computed and subsequent use of these components in understanding data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are principal components ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We wish to visualize n observations with measurements on a set of p features $X_1, ...X_p$ as a part of exploratory data analysis.\n",
    "- We could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations measurements on two of the features.\n",
    "- It will be difficult to observe all plots if the number of features are large.\n",
    "- We would like to find low-dimensional representation of data that captures most of the information.\n",
    "- The idea each of n observations live in p-dimensional space but not all of these dimensions are equally interesting.\n",
    "- Each of the dimensions found by PCA is a linear combination of the p features.\n",
    "- The first 'principal component' of a set of features $X_1, X_2,...X_p$ is the normalized linear combination of the features:\n",
    "\n",
    "    $Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + ... + \\phi_{p1} X_p$\n",
    "\n",
    "- that has the large variance.\n",
    "- We refer to $\\phi_{11}, ..\\phi_{p1}$ as the loadings of the first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a n x p data set X, we need to compute the first rpincipal component.\n",
    "- Since we are only interested in variance, we assume that each of the variables in X has been centered to have mean zero.\n",
    "- We then look for linear combination of sample feature values of the form:\n",
    "    \n",
    "    $z_{i1} = \\phi_{11} X_1 + \\phi_{21} X_2 + ... + \\phi_{p1} X_p$\n",
    "    \n",
    "- that has largest variance subject to constraint $\\sum_{j=1}^{p} \\phi_{j1}^{2} = 1$.\n",
    "- The first principal component loading vector solves the optimization problem:\n",
    "    \n",
    "    $maximize{\\frac{1}{n} \\sum_{i=1}^{n} (\\sum_{j=1}^{p} \\phi_{j1} x_{ij})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a nice geometric interpretation for the first principal component.\n",
    "- The loading vector $\\phi_1$ with elements $\\phi_{11}, ..\\phi_{p1}$ defines a direction in feature space along which the data vary the most.\n",
    "- If we project n data points $x_1, x_2...x_n$ into this direction, the projected values are the principal component scores $z_{11}, ...z_{n1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
