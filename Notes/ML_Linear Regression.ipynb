{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Response Variable : Y\n",
    "- Predictor Variable : X\n",
    "- Use test Y, X to estimate the coefficients $\\alpha$ and $\\beta$\n",
    "\n",
    "$Y_i$ = $\\beta$ $X_i$ + $\\alpha$ + $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Y\n",
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the error which is obtained when predicting y_i from beta * x_i + alpha when actual value is y_i\n",
    "def error(alpha, beta, x_i, y_i):\n",
    "    return y_i - predict(alpha, beta, x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to calculate the total error over the entire data set. But if we add all the errors we will probably get the wrong value since there may be positive error values and negative error values which will cancel each other. \n",
    "- To avoid this, we generally square each value and then add them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual sum of squares (RSS)\n",
    "def sum_of_squared_errors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2 for x_i, y_i in zip(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next step after calculating the total error is to minimize this error and have as good fit (of regression line) as possible.\n",
    "- This can be done by choosing the best alpha and beta values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(x,y):\n",
    "    \"\"\"Given training values of x and y,\n",
    "        find the least_squares values of alpha and beta\"\"\"\n",
    "    beta = correlation(x,y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The choice of alpha says that when we see the averagge value of x we predict the average value of y\n",
    "- The choice of beta says that when input value increases by standard_deviation(x), the prediction increases by correlation(xy) * standard_deviation(y).\n",
    "- When x and y are perfectly correlated, one standard_deviation increase in x results in one standard deviation increase of y in prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE\n",
    "\n",
    "- Calculating the square root of average of sum of squares of residuals (RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(alpha, beta, x, y):\n",
    "    return np.sqrt(sum_of_squared_errors(alpha, beta, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$ statistic\n",
    "\n",
    "- A better way to check the fit of the data is to calculate the coefficient of determination ($R^2$) which measures the fraction of total variation in the dependent variable  \n",
    "\n",
    "$R^2$ = (TSS - RSS) / TSS\n",
    "\n",
    "- where TSS = Total sum of squares, RSS = Residual sum of squares\n",
    "\n",
    "- TSS measures the total variance in the response Y , and can be thought of as the amount of variability inherent in the response before the regression is performed. \n",
    "- In contrast, RSS measures the amount of variability that is left unexplained after performing the regression.\n",
    "- Hence, TSS âˆ’ RSS measures the amount of variability in the response that is explained (or removed) by performing the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sum_of_squares(y):\n",
    "    \"\"\"total squared variation of y_i's from their mean\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))     # de_mean(y) : translate y by subtracting its mean\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    \"\"\"Fraction of variation captured by mode = 1 - fraction of variation not captured by model\"\"\"\n",
    "    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) / total_sum_of_squares(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The values for $R^2$ range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable. \n",
    "- A model with an $R^2$ of 0 is no better than a model that always predicts the mean of the target variable, whereas a model with an $R^2$ of 1 perfectly predicts the target variable. \n",
    "- Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
